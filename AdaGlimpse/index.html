<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description"
          content="Intelligently explore the environment visually just like humans do, based on internal model uncertainty.">
    <meta property="og:title"
          content="AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale"/>
    <meta property="og:description"
          content="Intelligently explore the environment visually just like humans do, based on internal model uncertainty."/>
    <meta property="og:url" content="https://io.pardyl.com/AME/"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/images/architecture.png"/>
    <!--  <meta property="og:image:width" content="753"/>-->
    <!--  <meta property="og:image:height" content="630"/>-->


    <meta name="twitter:title"
          content="AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale">
    <meta name="twitter:description"
          content="Intelligently explore the environment visually just like humans do, based on internal model uncertainty.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/architecture.png">
    <!--  <meta name="twitter:card" content="summary_large_image">-->
    <!-- Keywords for your paper to be indexed by-->
    <!--  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">-->
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Active Visual Exploration Based on Attention-Map Entropy</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">AdaGlimpse: Active Visual Exploration with Arbitrary
                        Glimpse Position and Scale</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                <a href="https://scholar.google.com/citations?user=vszbX_0AAAAJ&hl=pl"
                   target="_blank">Adam Pardyl</a><sup>1,2</sup>,</span>
                        <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=3s9uDh8AAAAJ&hl=pl&oi=ao"
                     target="_blank">Michał Wronka</a><sup>2</sup>,</span>
                        <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=f6Xi7aoAAAAJ&hl=pl&oi=ao" target="_blank">Maciej Wołczyk</a><sup>1</sup>,
                  </span>
                        <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=O30Xj14AAAAJ&hl=pl&oi=ao" target="_blank">Kamil Adamczewski</a><sup>1</sup>,
                  </span>
                        <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=bJMRBFoAAAAJ&hl=pl" target="_blank">Tomasz Trzciński</a><sup>1,3,4</sup>
                  </span>
                        <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=AJHaOpkAAAAJ&hl=pl" target="_blank">Bartosz Zieliński</a><sup>1,2</sup>,
                  </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>IDEAS NCBR,
                      <sup>2</sup>Jagiellonian University,<br>
                      <sup>3</sup>Warsaw University of Technology,
                      <sup>4</sup>Tooploox,<br>
                      ECCV 2024</span>
                    </div>


                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- poster PDF link -->
                            <!--                            <span class="link-block">-->
                            <!--                        <a href="static/pdfs/poster_ijcai_ame.pdf" target="_blank"-->
                            <!--                           class="external-link button is-normal is-rounded is-dark">-->
                            <!--                        <span class="icon">-->
                            <!--                          <i class="fas fa-window-maximize"></i>-->
                            <!--                        </span>-->
                            <!--                        <span>Poster</span>-->
                            <!--                      </a>-->
                            <!--                    </span>-->


                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                                <a href="https://arxiv.org/abs/2404.03482" target="_blank"
                                   class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Paper</span>
                                </a>
                            </span>

                            <!-- Supplementary PDF link -->
                            <!--                            <span class="link-block">-->
                            <!--                      <a href="static/pdfs/supplementary.pdf" target="_blank"-->
                            <!--                         class="external-link button is-normal is-rounded is-dark">-->
                            <!--                      <span class="icon">-->
                            <!--                        <i class="fas fa-file-pdf"></i>-->
                            <!--                      </span>-->
                            <!--                      <span>Supplementary</span>-->
                            <!--                    </a>-->
                            <!--                  </span>-->

                            <!-- Github link -->
                            <span class="link-block">
                                <a href="https://github.com/apardyl/AdaGlimpse" target="_blank"
                                   class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                          </a>
                            </span>
                            <!-- Github link -->
                            <span class="link-block">
                    <a href="https://huggingface.co/apardyl/AdaGlimpse/tree/main" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-download"></i>
                    </span>
                    <span>Checkpoints</span>
                  </a>
                </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Youtube video-->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered has-text-centered">
                <!-- Youtube embed code here -->
                <video width="100%" height="100%" autoplay="" muted="" controls="" loop="" preload="metadata"
                       aria-hidden="true">
                    <source src="static/images/preview.mp4" type="video/mp4" aria-hidden="true">
                    Your browser does not support the video tag.
                </video>

            </div>
            <h2 class="subtitle has-text-centered">
                Intelligently explore the environment visually, zooming-in on important features.
            </h2>
        </div>
    </div>
</section>
<!--End youtube video-->


<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Active Visual Exploration (AVE) is a task that involves dynamically selecting observations
                        (glimpses), which is critical to facilitate comprehension and navigation within an environment.
                        While modern AVE methods have demonstrated impressive performance, they are constrained to
                        fixed-scale glimpses from rigid grids. In contrast, existing mobile platforms equipped with
                        optical zoom capabilities can capture glimpses of arbitrary positions and scales. To address
                        this gap between software and hardware capabilities, we introduce AdaGlimpse. It uses Soft
                        Actor-Critic, a reinforcement learning algorithm tailored for exploration tasks, to select
                        glimpses of arbitrary position and scale. This approach enables our model to rapidly establish a
                        general awareness of the environment before zooming in for detailed analysis. Experimental
                        results demonstrate that AdaGlimpse surpasses previous methods across various visual tasks while
                        maintaining greater applicability in realistic AVE scenarios.
                    </p>
                </div>
            </div>

        </div>
    </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title">Adaptive Visual Exploration</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <!-- Your image here -->
                    <img src="static/images/teaser.png" alt="visual exploration - human vs. AI" class="my-5"/>
                    <p class="has-text-left">Our approach selects and processes
                        glimpses of arbitrary position and scale, fully exploiting the capabilities of modern
                        hardware. In this example, AdaGlimpse selects a low-resolution glimpse of the whole
                        environment. Based on this glimpse, it predicts a bird with probability 0.01, too low
                        to make the final decision. Instead, it selects the second glimpse by zooming in to the
                        upper left corner. The process repeats four times until the probability of the predicted
                        class is higher than a specified threshold.</p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title">AdaGlimpse</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <!-- Your image here -->
                    <img src="static/images/arch.png" alt="" class="my-5"/>
                    <p class="has-text-left">AdaGlimpse consists of two parts: a vision transformer-based
                        encoder with a task-specific head and a Soft Actor-Critic RL agent. At each exploration step,
                        the RL agent selects the position and scale of
                        the next glimpse based on the information about previous patches, their coordinates,
                        importance, and latent representations.</p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title">Efficient image recognition</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <!-- Your image here -->
                    <img src="static/images/cls.png" alt="" class="my-5"/>
                    <p class="has-text-left">Active visual exploration with AdaGlimpse can be used to quickly recognise
                        objects in partially observable scenes using only a fraction of maximal image resolution.
                        In this example, AdaGlimpse explores 224 × 224 images from
                        ImageNet with 32 × 32 glimpses of variable scale, zooming in on objects of interest and
                        stopping the process after reaching 75% predicted probability. The model uses respectively only
                        20 and 8 transformer patches, where standard vision transformer would require 196 patches. The
                        rows correspond to: A) glimpse locations, B) pixels visible to the model (interpolated from
                        glimpses for preview), C) predicted label, D) prediction probability.</p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title">Quick scene understanding</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <!-- Your image here -->
                    <img src="static/images/rec.png" alt="" class="my-5"/>
                    <p class="has-text-left">Robotic agents face limitations in sensor and processing capabilities. By
                        intelligently choosing areas to explore we get awareness of the environment faster. In this
                        example, AdaGlimpse explores 224 × 224 images from MS COCO with 16 × 16 glimpses of variable
                        scale, zooming in on objects of interest. Note, that each glimpse consists of a single vision
                        transformer patch. The model reconstructs full resolution scene using only 12 patches, where a
                        standard vision transformer would require 196 patches.
                        The columns correspond to: A) glimpse locations, B) pixels visible to the model
                        (interpolated from glimpses for preview), C) reconstruction result.</p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title">Benchmark: scene reconstruction</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <!-- Your image here -->
                    <img src="static/images/benchrec.png" alt="" class="my-5"/>
                    <p class="has-text-left">Reconstruction results: RMSE (lower is better) obtained by our model for
                        reconstruction task against AttSeg, GlAtEx, SimGlim, and AME on
                        ImageNet-1k, SUN360, ADE20K and MS COCO datasets. Regardless of the number
                        of glimpses, as well as their resolution and regime, our method outperforms
                        competitive solutions. Note that Pixel % denotes the percentage of image pixels known
                        to the model, † a reproduced result not published in the relevant paper, and * zero-shot
                        performance.</p>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title">Benchmark: image classification</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <!-- Your image here -->
                    <img src="static/images/benchcls.png" alt="" class="my-5"/>
                    <p class="has-text-left">Classification results: Accuracy obtained by our model for classification
                        task against DRAM, GFNet, Saccader, STN, TNet, PatchDrop
                        and STAM on ImageNet-1k dataset. Our AdaGlimpse needs 40% less pixels to
                        match the performance of the best baseline method. Note that Pixel % denotes the
                        percentage of image pixels known to the model, and regimes are described in the paper.</p>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- poster PDF link -->


                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                        <a href="https://arxiv.org/abs/2404.03482" target="_blank"
                           class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Read the paper</span>
                      </a>
                    </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{pardyl2024adaglimpse,
  title     = {AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale},
  author    = {Pardyl, Adam and Wronka, Michał and Wołczyk, Maciej and Adamczewski, Kamil and Trzciński, Tomasz and Zieliński, Bartosz},
  booktitle = {Computer Vision -- ECCV 2024},
  publisher = {Springer Nature Switzerland},
  year      = {2024},
  note      = {Main Track},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic
                        Project Page Template</a>.
                        You are free to borrow the of this website, we just ask that you link back to this page in the
                        footer. <br> This website is licensed under a <a rel="license"
                                                                         href="http://creativecommons.org/licenses/by-sa/4.0/"
                                                                         target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
